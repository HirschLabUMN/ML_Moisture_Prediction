---
title: "Validation Cooks Analysis"
author: "Michael Burns"
date: "3/2/2020"
output: html_document
---

This R markdown is to analyze the machine learning valiation from the cook tests that have been performed.  This I am interested in are the number of samples that have been cooked (singly or doubly), the correlation between the predicted and actual moisture uptake values, to trend between error and either predicted or actual, and the RMSE of the model overall.


```{r loading}
dataset <- read_xlsx("../Data/ML_Validation_Cook_Tests_MBurns.xlsx") %>%
  filter(!str_detect(string = SampleID, pattern = "Test"))
head(dataset)
```

```{r counting}
dataset %>% 
  select(c("Genotype", "SampleID", "Moisture.Avg")) %>%
  filter(Moisture.Avg != "NA" & Moisture.Avg !="#DIV/0!" & Moisture.Avg < 0.7) %>% 
  group_by(SampleID) %>% 
  summarise(n = n()) %>% 
  ungroup() %>% 
  summarise(sum_1 = sum(n == 1),
            sum_2 = sum(n == 2),
            sum_tot = sum_1 + sum_2)
```
Looks like we have completed 36 of the 40 genotypes, and of those, 20 have been cooked twice.

```{r combining_like_samples}
dataset_combined <- dataset %>%
  select(c("Genotype", "SampleID", "Y.Kernel.Moisture", "Z.Kernel.Moisture", "Moisture.Pred")) %>%
  filter(!is.na(Moisture.Pred)) %>%
  filter(Y.Kernel.Moisture != "#DIV/0!") %>%
  mutate(Y.Kernel.Moisture = as.numeric(Y.Kernel.Moisture),
         Z.Kernel.Moisture = as.numeric(Z.Kernel.Moisture)) %>%
  group_by(SampleID) %>%
  summarise(Genotype = unique(Genotype),
            n = n(),
            Moisture_Actual = mean(c(Y.Kernel.Moisture, Z.Kernel.Moisture)),
            Moisture_Pred = mean(Moisture.Pred),
            Moisture_Error = Moisture_Pred - Moisture_Actual) %>%
  filter(Moisture_Actual < 0.7)
```

```{r rmse}
dataset_combined%>%
  summarise(RMSE = sqrt(mean(Moisture_Error^2)))
```
The RMSE is okay, but not great.  Definitely higher than expected from the training.
WORRY MORE ABOUT RELATIVE RANK RATHER THAN RMSE.  SPEARMAN RANK -- RELATIVE SCALE.


```{r actual_vs_predicted}
dataset_combined %>%
  ggplot(aes(x = Moisture_Actual, y = Moisture_Pred))+
  geom_point()+
  ylab('Moisture Uptake Prediction')+
  xlab('Actual Moisture Uptake')+
  labs(title = 'Moisture Uptake Prediction vs Actual Moisture Uptake')

dataset_combined %>%
  ggplot(aes(x = Moisture_Actual, y = Moisture_Pred))+
  geom_text(aes(label = Genotype))+
  ylab('Moisture Uptake Prediction')+
  xlab('Actual Moisture Uptake')+
  labs(title = 'Moisture Uptake Prediction vs Actual Moisture Uptake')

dataset_combined %>%
  ggplot(aes(x = Moisture_Actual, y = Moisture_Pred))+
  geom_text(aes(label = SampleID))+
  ylab('Moisture Uptake Prediction')+
  xlab('Actual Moisture Uptake')+
  labs(title = 'Moisture Uptake Prediction vs Actual Moisture Uptake')
```
Things seem pretty well correlated.  There are three samples though that sort of seem to stick out.  E8501 could be part of the spread still.
LOOK AT THE GROUPING IN THE MIDDLE BOTTOM TO SEE IF THEIR SPECTRA ARE OUTSIDE OF RANGE OF TRAINING SET.
LOOK AT THEIR GERMPLASM TYPE

```{r looking_at_germplasm_type}
dataset_combined %>%
  filter(Moisture_Pred < 0.35 & Moisture_Actual > 0.39)
```
LOOK TO SEE IF THESE ARE DIFFERENT TYPES OF GERMPLASM FROM NO. 2 DENT.
From what I have found, E8501 is non-stiff stalk (also the closest one to the cluster), Inbred 305 is a tropical germplasm (furthest from cluster), and B77 is lancaster (second furthest from cluster).


```{r error_dynamic_range}
dataset_combined %>%
  select(c("Moisture_Actual", "Moisture_Pred", "Moisture_Error")) %>%
  pivot_longer(cols = -Moisture_Error, names_to = "Pred_or_Avg", values_to = "Value") %>%
  ggplot(aes(x = Value, y = Moisture_Error, color = Pred_or_Avg))+
  geom_smooth(se = F)+
  ylab("Error")+
  xlab("Moisture Value")+
  labs(title = "Error vs Moisture Value (Predicted and Actual)")+
  scale_color_discrete(name = 'Line Classification', labels = c('Actual Moisture Uptake','Predicted Moisture Uptake'))
```
Based on this graph, our model seems to be best around 0.4 to 0.48.  We may be able to use it outside of this if we use ranked samples.  For example, if we have two that we know are at the bounds of ideal moisture uptake, all we would need to test for is that it is above the lower one, and below the upper.


```{r spread_of_error}
dataset_combined %>%
  ggplot(aes(x = Moisture_Error))+
  geom_histogram(binwidth = 0.015)
```
Error definitely looks left skewed.

Correlation of the points that fit the trend
```{r}
cor_dataset <- dataset_combined %>%
  filter(Moisture_Pred > 0.32)
cor(x = cor_dataset$Moisture_Actual, y = cor_dataset$Moisture_Pred)
cor(x = cor_dataset$Moisture_Actual, y = cor_dataset$Moisture_Pred)^2
cor(x = cor_dataset$Moisture_Actual, y = cor_dataset$Moisture_Pred, method = "spearman")
cor(x = cor_dataset$Moisture_Actual, y = cor_dataset$Moisture_Pred, method = "spearman")^2

cor_dataset %>%
  ggplot(aes(x = Moisture_Actual, y = Moisture_Pred))+
  geom_point()+
  geom_smooth(method = "lm")+
  ylab('Moisture Uptake Prediction')+
  xlab('Actual Moisture Uptake')+
  labs(title = 'Moisture Uptake Prediction vs Actual Moisture Uptake')
```
0.74 is a pretty good correlation to have.






___
___
___
This section is just for testing.  None of it is intended to inform decisions.  I was simply curious what a statistical test would say about the difference between actual and predicted values.
```{r paired_t_test_assumptions}
dataset_combined %>%
  select(c("Moisture_Actual", "Moisture_Pred", "Moisture_Error")) %>%
  ggplot(aes(sample = Moisture_Error))+
  geom_qq()+
  geom_qq_line()
```
Kind of normalish, I guess?  Except for extremes.

```{r}
t.test(dataset_combined$Moisture_Pred, dataset_combined$Moisture_Actual, paired = TRUE)
```





