---
title: "New NIR Model Equations"
author: "Michael Burns"
date: "12/20/2019"
output: html_document
---
The purpose of this rmarkdown file is to analyze the trait data we have and see if we can explain large portions of variance in them with the spectral data.  One thing to keep in mind is that some may have larger variances explained than others due to the nature of their bonds.  NIR primarily works on H bonds, and so it is possible Ash might not be very well explained, or that some important/unique bonds wont be quantified such as how protein is normally quantified by their aromatic amino acid groups.

```{r}
library(pls)
library(dplyr)
library(ggplot2)
library(reshape2)
library(Hmisc)
library(progress)
library(readxl)
```

```{r}
trait_spec_data<-read_csv("../Data/Spectra_Data/Trait_and_Spectral_Data.csv")
head(trait_spec_data)
```

In theory, one should be able to extract number of components from the PLS code and can more or less run a giant loop that will create spectra based predictions for all of these traits.  We may want to take out about 20% for testing though.  Perhaps 10% would suffice?

```{r}
trait_spec_data_index<-caret::createDataPartition(trait_spec_data$Ash.Avg, p=0.80, list = FALSE)
trait_spec_data_test<-trait_spec_data[-trait_spec_data_index,]
trait_spec_data_train<-trait_spec_data[trait_spec_data_index,]
```


```{r}
training_spectra_readings<-as.matrix(trait_spec_data_train[,c(12:152)]) #having the spectra in a matrix will allow a cleaner code when calling them as predictors.
dim(training_spectra_readings)

test_spectra_readings<-as.matrix(trait_spec_data_test[,c(12:152)]) #having the spectra in a matrix will allow a cleaner code when calling them as predictors.
dim(test_spectra_readings)
```

```{r}
spectra_model<-plsr(trait_spec_data_train$Ash.Avg~training_spectra_readings, validation = "CV")
```

```{r}
R2_Y<-R2(spectra_model, estimate = "train")
R2_Y_max<-max(R2_Y$val)
R2_Y_mincomp<-R2_Y$val<0.8*R2_Y_max
R2_Y_compnum<-sum(R2_Y_mincomp)+1

RMSEP_mod<-RMSEP(spectra_model)
RMSEP_val<-RMSEP_mod$val
RMSEP_val_CV<-RMSEP_val[c(T,F)]
RMSEP_min<-which.min(RMSEP_val_CV)-1 # -1 corrects for intercept being considered in the RMSEP values list

plot(RMSEP_mod, legendpos = "topright", 
     main = "RMSE vs Number of Components Used", 
     ylab = "RMSE",
     xlab = "Number of Components",
     sub = paste("Minimum RMSE found with", RMSEP_min, "components", sep = " "),
     col.sub = "blue")
abline(v = RMSEP_min, col = "blue")

plot(R2_Y, 
     main = "R^2 vs Number of Components Used", 
     ylab = "R^2",
     xlab = "Number of Components",
     sub = paste(round(0.8*R2_Y_max*100,1), "% of variation found with ", R2_Y_compnum, " components", sep = ""),
     col.sub = "blue")
abline(v = R2_Y_compnum, col = "blue")

ncomp_onesigma<-selectNcomp(spectra_model, 
                            method = "onesigma", 
                            plot = T, 
                            xlim = c(0,15), 
                            sub = paste("Variance Explained: ", 
                                        round(R2_Y$val[selectNcomp(spectra_model,
                                                                   method ="onesigma")+1],3)*100,"%", sep =""),
                            col.sub = "blue")

ncomp_permut<-selectNcomp(spectra_model, 
                          method = "randomization", 
                          plot = T, 
                          xlim = c(0,15),
                          sub = paste("Variance Explained: ", 
                                      round(R2_Y$val[selectNcomp(spectra_model,
                                                                 method ="randomization")+1],3)*100,"%", sep =""),
                          col.sub = "blue")
# +1's corrects for intercept being included in the R^2 dataset.
```
It appears that at least for Ash.Avg The spectra can explain all of the variation.  We might want to consider moving the R2 % value up to get an even more accurate model, though this could limit predictive power.

```{r}
component_predictions_pls<-drop(predict(spectra_model, ncomp = 18, newdata = test_spectra_readings))
```

```{r}
Trait_Data_From_Equations_Testing<-function(dataset,column_of_interest, p = 0.90, metric = "RMSE", R2_target = 0.8, validation = "CV"){
  #####
  # Loading dataset and partitioning
  #####
  trait_spec_data<-dataset
  trait_spec_data_index<-caret::createDataPartition(trait_spec_data[[column_of_interest]], p=p, list = FALSE)
  trait_spec_data_test<-trait_spec_data[-trait_spec_data_index,]
  trait_spec_data_train<-trait_spec_data[trait_spec_data_index,]
  
  #####
  # Creating spectra matrices
  #####
  training_spectra_readings<-as.matrix(trait_spec_data_train[,c(12:152)]) #having the spectra in a matrix will allow a cleaner code when calling them as predictors.
  dim(training_spectra_readings)
  test_spectra_readings<-as.matrix(trait_spec_data_test[,c(12:152)]) #having the spectra in a matrix will allow a cleaner code when calling them as predictors.
  dim(test_spectra_readings)
  
  #####
  # Creating the model
  #####
  spectra_model<-plsr(log(trait_spec_data_train[[column_of_interest]])~training_spectra_readings, validation = validation)
  
  #####
  # Metrics for determining number of components to use
  #####
  R2_Y<-R2(spectra_model, estimate = "train")
  R2_Y_max<-max(R2_Y$val)
  R2_Y_mincomp<-R2_Y$val<R2_target*R2_Y_max
  R2_Y_compnum<-sum(R2_Y_mincomp)+1
  RMSEP_mod<-RMSEP(spectra_model)
  RMSEP_val<-RMSEP_mod$val
  RMSEP_val_CV<-RMSEP_val[c(T,F)]
  RMSEP_min<-which.min(RMSEP_val_CV)-1 # -1 corrects for intercept being considered in the RMSEP values list
  
  remsp_plot<-plot(RMSEP_mod, legendpos = "topright", 
     main = "RMSE vs Number of Components Used", 
     ylab = "RMSE",
     xlab = "Number of Components",
     sub = paste("Minimum RMSE found with", RMSEP_min, "components", sep = " "),
     col.sub = "red")
  abline(v = RMSEP_min, col = "red")
  abline(v = R2_Y_compnum, col = "blue")
  
  r2_plot<-plot(R2_Y, 
     main = "R^2 vs Number of Components Used", 
     ylab = "R^2",
     xlab = "Number of Components",
     sub = paste(round(0.8*R2_Y_max*100,1), "% of variation found with ", R2_Y_compnum, " components", sep = ""),
     col.sub = "blue")
  abline(v = R2_Y_compnum, col = "blue")
  abline(v = RMSEP_min, col = "red")
  
  remsp_plot
  r2_plot
  
  cat("Number of Components Based on R^2: \n", R2_Y_compnum, "\n","Number of Components Based on RMSEP: \n", RMSEP_min, sep = "")
  ncomp<-as.numeric(readline(prompt = "Number of Components to Use:"))
  
  #####
  # Prediction model
  #####
  component_predictions_pls<-drop(predict(spectra_model, ncomp = ncomp, newdata = test_spectra_readings))
  component_predictions_pls_transformed<-exp(component_predictions_pls)
  
  #####
  # Which metric to print
  #####
  if(metric == "RMSE"){
  sqrt(mean((trait_spec_data_test[[column_of_interest]]-component_predictions_pls_transformed)^2))
  }
    else{
      if(metric == "R2"){
        caret::R2(component_predictions_pls_transformed, trait_spec_data_test[[column_of_interest]])
      }
        else{print("Unsupported metric, choose either RMSE or R2")}
    }
}
```
It appears that we can explain 100% of the variation in the dataset with at some point with the spectra.  We can try increasing the R2 parameter to increase the number of components used. This could give us either a better model, or a much worse one.









___
___
___
Trials of hyperparameters
```{r}
RMSE_reps_p <- vector(length = 20)
RMSE_mean_data_p <- vector(length = 46)

for(p in 50:95){
  for(i in 1:20){
    RMSE_reps_p[i]<-Trait_Data_From_Equations_Testing(trait_spec_data,3, p = p/100, metric = "RMSE")
  }
  RMSE_mean_data_p[(p-49)]<-mean(RMSE_reps_p)
}
plot(RMSE_mean_data_p, main = "Average RMSE of 20 trials at various partition values", xaxt="n", ylab = "Average RMSE", xlab = "Partitioning Value")
axis(1, at=c(0,10,20,30,40), labels=c(0.50,0.60,0.70,0.80,0.90))
```
It looks like the mean RMSE starts to drop around p = 70, and the mean SD begins to rise around p = 70.

```{r}
RMSE_reps_r <- vector(length = 50)
RMSE_mean_data_r <- vector(length = 40)

for(r in 60:99){
  for(i in 1:50){
    RMSE_reps_r[i]<-Trait_Data_From_Equations_Testing(trait_spec_data,3, p = 0.8, metric = "RMSE", R2_target = r/100)
  }
  RMSE_mean_data_r[(r-59)]<-mean(RMSE_reps_r)
}
plot(RMSE_mean_data_r, main = "Average RMSE of 50 trials at various R^2 target values",xaxt = "n", ylab = "Average RMSE", xlab = "R^2 Target Value")
axis(1, at=c(0,10,20,30,40), labels=c(0.6,0.7,0.8,0.9,0.99))
```
Mean RMSE definitely appears to increase with increased R2 values used for components. SD of RMSE appears to have a slight decline as R2 increases, but its hard to tell.

Lets look at how number of components changes the RMSE.  
```{r}
RMSE_reps_c <- vector(length = 50)
RMSE_mean_data_c <- vector(length = 25)
for(c in 1:25){
  for(i in 1:50){
    RMSE_reps_c[i]<-Trait_Data_From_Equations_Testing(trait_spec_data,3, p = 0.9, metric = "RMSE", R2_target = 0.8, ncomptest = c)
  }
  RMSE_mean_data_c[(c)]<-mean(RMSE_reps_c)
}
RMSE_mean_data_c
plot(RMSE_mean_data_c, main = "Average RMSE of 50 trials at various ncomp values (p = 0.8, r2 = 0.8)", ylab = "Average RMSE", xlab = "Ncomp Value")
```

RMSE seems to decrease, if not stay constant, at R2 targets below 0.8.  It also seems to decrease with increased partitioning values, and it seems to have a low spot around 10-12 components.  Is there a way we can incorporate the lower number of components with the smaller R2 value?  As long as the RMSE stays relatively constant below an R2 of 0.8, I supposed we could choose any number of components.

R2 does stay constant below ~0.8, so any ncomp that falls below that is a safe one to choose.

It looks like it would be beneficial to log transform the data before making the model.  Will need to work on that.

It looks like taking the log transformation of the trait values before learning, and then retransforming them back after predictions might have helped. I am going to try to created some vectors to try to prove whether that is true or not.
```{r}
RMSE_reps_t <- vector(length = 50)
RMSE_mean_data_t <- matrix(nrow = 50, ncol = 10)
for(t in 2:11){
  for(i in 1:50){
    RMSE_reps_t[i]<-Trait_Data_From_Equations_Testing(trait_spec_data,3)
  }
  RMSE_mean_data_t[,(t-1)]<-RMSE_reps_t
}
boxplot(RMSE_mean_data_t, main = "Average RMSE of 50 trials through log transformation", xaxt = "n", ylab = "Average RMSE", xlab = "Column")
axis(1, at = c(1:10), labels = c(2:11))
```


```{r}
RMSE_reps_v <- vector(length = 50)
RMSE_mean_data_v <- matrix(nrow = 50, ncol = 2)
for(t in 1:2){
  if(t == 1){
    for(i in 1:50){
      RMSE_reps_v[i]<-Trait_Data_From_Equations_Testing(trait_spec_data,3, validation = "LOO")
    }
    RMSE_mean_data_v[,t]<-RMSE_reps_v
  }
  else{
  for(i in 1:50){
    RMSE_reps_v[i]<-Trait_Data_From_Equations_Testing(trait_spec_data,3, validation = "CV")
  }
    RMSE_mean_data_v[,t]<-RMSE_reps_v
  }
}
boxplot(RMSE_mean_data_v, main = "Average RMSE of 50 trials with LOO vs CV", xaxt = "n", ylab = "Average RMSE", xlab = "Column")
axis(1, at = c(1,2), labels = c("LOO", "CV"))
```

The normal CV validation method is technically no different than the LOO option, but since the average was lower, I will continue with that.


